= Solution Addressing The Challenges
:navtitle: 2: Addressing Challenge
:numbered:

== Addressing Challenge with AI/ML and MLOps (GitOps):
Product Manager (PM) for clothing category along with Marketing Team reviews sentiment analysis and respective comments.
They conclude that cost seems to be a major issue for sentiments and comes up with a discounting offer.
PM approaches data scientist team to come up with an object detection based discount coupon solution.

* Customer goes to a store app or online to take picture of the product which they want to buy.
* Based on the Machine Learning logic, object is detected and a discount is offered.
* Customer keys in this discount online or shows to the Shop Manager for discount.
* Customer avails discount which eventually will turn the sentiments positive.

. Accessing and testing your deployed Application

+
****
[upperalpha]

An initial instance of your application was deployed during the lab environment creation.
Let’s review it and confirm it works as expected.

[arabic]
. Follow the URL marked as `Discount Application url'
* *Discount Application url:* https://object-detection-app-git-retail-rhods-project.{openshift_cluster_ingress_domain}[window=_blank]

. This will open the app in your browser
. You will be prompted to authorize your browser to use your Camera.
Allow it to do so.
. When you take a picture that contains either a piece of clothing,
footwear, or a bottle it should display a rebate overlaid on the image.
. You can also send the URL to your smart phone in order to have a more
portable camera than your laptop
****

. Retraining the model

.. The initial version of the discounting model was built using a dataset
called link:{gitea_console_url}/lab-user/arc-model/src/branch/main/discount_data/datasets/monday.csv[monday.csv].

.. In this section, we will:
... log into Red Hat OpenShift AI
... Clone the project from on-cluster gitea instance (git repo)
... Retrain the model with fresher data
... Publish our changes

.. Logging into Red Hat OpenShift AI

+
****

... Red Hat OpenShift AI URL: {rhodh_dashboard}[window=_blank]
... Click on `Log in with OpenShift`
... Log in as `user1` with the password `openshift`
... Click *Allow Selected Permissions* to Authorize the access
... Once in the Red Hat OpenShift AI Dashboard, Click on the *Launch Application*
hyperlink in the JupyterHub tile
... Log in as `user1` with the password `openshift`
... Click *Allow Selected Permissions* to Authorize the access
... Choose *Standard Data Science* as the image and *Small* as the
Notebook Size
... Click on *Start Server*. Wait for the process to finish starting the notebook server.
Since its for the 1st time the notebook image is pulled and hence it take time to start the notebook.
... Once the process is completed, click on `Open in new tab`
... This will start a new notebook tab.
... Again login using the same user1 / openshift as the username and password, Allow all permissions should take you to the juypterhub notebook server UI.
****

+
.. Next is to clone the arc-model project repository in the jupyterhub notebook server you just started.

+
****
... Each environment comes with a dedicated instance of Gitea so that each
student can easily and independently make updates in the git repo
without affecting the others.

... The Gitea URL to use should be provided together with your environment
information here:
... *Gitea URL:* {gitea_console_url}[window=_blank]
... Sign in to Gitea as user `lab-user` with password `openshift`.
... Navigate to the *arc-model* git repo.
... Copy the Git Clone URL: {gitea_console_url}/lab-user/arc-model.git
** image:copy-git-url.png[copy-git-url]
... Navigate back to your JupyterLab tab
... Click on the Git Icon (fourth of the 6 big icons on the left menur bar). See the image.
** image:clone-repo.png[clone-repo]
... Click on the *Clone a repository* button (Again see the image above)
... Paste the URL and click *Clone*
... In the File Explorer menu (left side) you will see arc-model folder created, double-click on *arc-model* to move to that
directory
****

+
.. Retrain the model
+
****
... Now that we’ve cloned the project, let’s retrain the model. Currently we are in the arc-model folder on our jupyterhub notebook server.
You can get more details about jupyterhub notebook here : https://jupyter-notebook.readthedocs.io/en/latest/[_JupyterHub Notebook_,window=_blank]

... From the left side tab, Open the Notebook called `5_discount_model.ipynb` from the Red Hat OpenShift AI jupyterhub notebook server UI which you got in earlier section.
** image:open-notebook-05.png[open notebook]
... This notebook contains the code for training our discount model.
... In the third cell you can see a reference to our `monday` dataset
called `discount_data/datasets/monday.csv`
... Update the content of the cell so that it points to our second
dataset: `discount_data/datasets/tuesday.csv`
... Select the *Run* menu, then the option: *Restart Kernel and Run
All Cells*
** image:csv-restart-run-all.png[csv-restart-run-all]
... Confirm by clicking *Restart*
... Once all the cells have run, you will notice that some of the files in
the folder *5_discount_models* have been updated. This is what you will also see as output of running cell 15.
** image:cell15_output.png[Cell 15 output]
****

+
.. Publish the changes

+
****
... We have now updated our model files as well as the notebook that was
used to generate them. We will push those changes back into our gitea
instance, in the `main` branch.

... Open up the notebook called `6_git_commit_and_push.ipynb`
... Once again, run the *Restart Kernel and Run all Cells*
... Doing this will automatically Commit our changes into the local git
repo, and then push those commits back into the Gitea instance.
****

Point to note:: Make a note that in this case, we are storing both the code (notebook) and model
(*.pkl) in Git. If this were a real production project, we’d probably
have a more advanced way of storing the various versions of the model.
Check the next section for model serving to get some idea on how its done.

. Reviewing the OpenShift Pipeline

.. In the previous steps, we pushed our changes back into the Gitea repo.

.. In this environment, an OpenShift pipeline has been configure to
automatically run every time something is pushed to Gitea.

... Reviewing the pipeline run

+
****
Our dev app should automatically rebuild since that we’ve pushed our
changes to the git repository.

[arabic]
. Follow the link to your `OpenShift Console URL' and login.
+
*****
.. Find the login details for accessing Openshift Console below:
... *Console URL:* {openshift_console_url}[window=_blank]
... *Username:* {openshift_cluster_admin_username}
... *Password:* {openshift_cluster_admin_password}
*****
. Select `Administrator` view.
image:select-administrator.png[select-administrator]
. Navigate to *Pipelines* , then *Pipelines* (yes, again), and then go
to *PipelineRuns*
image:select-pipelines.png[select-pipelines]
. Make sure that the selected project is *retail-rhods-project*
. You should see a pipeline run that failed on the third step
. Review the failed step.
image:pipeline_sanity_fail.png[Pipeline Run Sanity Check Fail]
. Our sanitycheck.py program is a safeguard that ensures the discounts
are never more than a certain percentage.
. It would seem that the new version of the model might be too generous
with the discount!
****

. Retrain the model (again).

.. Let’s fix this! Clearly we had a problem with our data - luckily we
received the data from wednesday which our data engineers have promised
will be correct.

.. Even more lucky, our pipeline has prevented us from putting a `bad`
model into our dev environment. Therefore, we don’t even need to worry
about rolling back a bad change: the bad change was prevented from
happening.

+
****
[arabic]
. Again, go to `5_discount_model.ipynb` notebook in your Red Hat OpenShift AI tab.
. Let’s use the new data from wednesday, update that same cell as before
to now point to `discount_data/datasets/wednesday.csv`.
. Now, rerun the notebook by clicking *Restart Kernel and Run All* as we
did before.
. This will update the discount model with a new discount model trained
on wednesday’s data. Wait till the last cell is executed and you see the output.
****

.. We could also run the sanity-check here, but the pipeline will take care
of that for us.

+
****
[arabic, start=5]
* Run the notebook `6_git_commit_and_push.ipynb` again to commit and
push our model changes to our git repo.
****

. Watch the build.

.. Let’s look at the pipeline build now that we’ve retrained our model with
what should be good data.

+
****
[arabic]
. Navigate back to your OpenShift Console tab.
+
*****
.. Find the login details for accessing Openshift Console below:
... *Console URL:* {openshift_console_url}[window=_blank]
... *Username:* {openshift_cluster_admin_username}
... *Password:* {openshift_cluster_admin_password}
*****
. Again, take a look at the PipelineRuns and click on the latest run
which should be in progress.
. We can click on the sanity check step within our pipeline, view the
log and see that the model has now passed our predefined tests.
image:pipeline_sanity_pass.png[Pipeline Run Sanity Check Pass]

. After the sanity check passes, the rest of the pipeline can now
complete and our app will be redeployed with our changes.
image:pipeline_run_successful.png[Pipeline Run completed successfully]
****

. Reviewing ArgoCD and GitOps

.. In the previous section, we’ve seen how the pipeline can help detect
potential issues and prevent from implementing `broken` artifacts in
our dev environment.

.. In this section, we will see how OpenShift GitOps is used deploy our
application, and then to maintain its state.

+
****
.. Connecting to OpenShift GitOps
... *OpenShift GitOps / ArgoCD URL*: https://{gitops_argocd_url}[window=_blank]
... *Username:* {gitops_argocd_username}
... *Password:* {gitops_argocd_admin_password}

.. Login to ArgoCD:
* When you first open up that URL, ≈you may get a warning that `your
connection is not private`.
* Click on *Advanced* and then *Proceed to
openshift-gitops…….opentlc.com(unsafe)*.
* You will use the username `admin` and the associated password provided above.
* Once you’re logged into ArgoCD, explore the 2 apps that you see (Red Boxes in the below image).
image:argocd_apps.png[ArgoCD Applications]
****

. Attempting a manual change in OpenShift

.. One way to illustrate the benefits of ArgoCD is to try to perform an
ad-hoc change in OpenShift.
+
****
* Open the OpenShift Console.
.. Find the login details for accessing Openshift Console below:
... *Console URL:* {openshift_console_url}[window=_blank]
... *Username:* {openshift_cluster_admin_username}
... *Password:* {openshift_cluster_admin_password}
* Navigate to *Workloads* and then *Deployments*.
* You will see that the deployment called `object-detection-rest`
currently has a single pod (replica)
image:object_detection_one_pod.png[Single pod object detection deployment]
* Select the Deployment (left hand side menu), you will see three deployment. If you click on the 3-dots icon for `object-detect-rest` deployment, you can
choose to *Edit pod count* or alternatives from your previous step you can increase the pod count using the up arrow to 5.
** image:edit_three_dots_deployment.png[Deployment pod Update using Edit option]
* Change that `1` into a `5` and click *Save*
* Nativage back to the object-detection-rest deployment by selecting that deployment and you will see that the pods are scaled up to 5.
** image:object_detection_five_pod.png[Multi pod object detection deployment]
****
.. By default, ArgoCD will reconcile things every 5 minutes. You can autosync as well,
but for learning purpose we have not used autosync here.
+
.. In the interest of time, we can trigger this to happen sooner. Let’s see
how from ArgoCD UI.
+
****
... Click on the *retail-dev* app
... Once the app is open, click on *APP DIFF*
... Tick the box that says *Compact Diff*

... The difference that you see should make sense
image:argocd_app_diff.png[ArgoCD application difference]

... Click on *Sync* * click *SYNCHRONIZE*
image:argocd_app_sync.png[ArgoCD Application Synchronize]
****

.. You will see that doing so will reset things to their original values.
The diff will go away, and the number of pods for this deployment will
go back down to 1.

.. In fact, you could actually delete a whole lot of things on the
OpenShift side for the specific managed apps, and ArgoCD would re-create them almost as quickly!

. Updating things the GitOps way

.. So if we did want more replicas, what we have to do is to do it in the
Gitea repo, and then get Argo to make that change happen. So let’s do
that.

+
****
* Access gitea again : *Gitea URL:* {gitea_console_url}[window=_blank]
* Make sure you are logged in as `lab-user` with password `openshift`
* Navigate to the repo called `retail-dev-gitops`
* In this repo, stay in the `main` branch
* Navigate to the file `/base/object-detection-rest-deployment.yaml`
* Edit the file directly in Gitea (using the pencil icon)
* Change the text `replicas: 1` to `replicas: 4`
* Commit the change with a meaningful commit message. For example:
image:commit-msg.png[commit-msg]
* Once that is done, toggle over to Argo and get it to refresh again.
* You will quickly see that the number of pods will have been changed in
the target environment as well.
****

.. Well, we’ve finally achieved our change, and it’s been implemented in
the cluster. As a bonus, we now have very good traceability on who did
that change when, and it’s also a lot easier to undo it if needed. Now this
was all about MLOps and how its achieved using GitOps methodology.
Of course you can always add security at every stage of your pipeline.

. Let's review the business side and see how this addresses the challenge.
.. We will introduce multiple comments using a simulator, link provided below:

+
****
... *Globex Review Simulator URL:* https://reviews-simulator-sentiment-analysis.{openshift_cluster_ingress_domain}[window=_blank]
... Go to your Globex review simulator to introduce multiple comments simultaneously as shown in the below image as POST, select clothing catalogue, then `Try it out` and Execute.
image:review-simulator.png[Product Review Simulation]

... Ensure that you execute the above step 4 to 5 times to generate enough comments for clothing category.This is what you see after Execution of the POST is successful. This will ensure that you have more than 60% positive sentiments and achieve the required KPI.
image:review-simulator-success.png[Product Review Simulation Successful Output.]
****

+
.. This will alter the grafana dashboard with more positive sentiment messages.
.. Find the login details for accessing Grafana Dashboard below:
+
****
... *Grafana Dashboard:* https://grafana-route-influxdb-project.{openshift_cluster_ingress_domain}[window=_blank]
... *Username:* admin
... *Password:* graphsRcool
****
+
.. Finally review the grafana dashboard showing better positive sentiments for clothing category.
+
****
image:improved_positive_sentiments.png[Improved Positive Sentiments.]
****
+
.. So a marketing strategy combined with object detection based discount coupon app, leading to better positive comments and sentiments as customer's are happy with these discount offers.
+
.. You can try similar comments for Bags category as well.

.. This concludes the solution addressing the challenge being presented in the previous section.

== Summary:
This section demonstrates how a team of data scientist can follow MLOps using GitOps methodology even for model creation and serving.
We also saw how GitOps methodology ensure that not only application and data science models, but even the deployments can follow single source (git repo) and being completely auditable.
It also ensure that the deployments are never deviated from what is being declared in the git infrastructure code and freeing up the customer's operation team's time for more innovative work.

*Let's move forward to the next page where we'll delve into the Red Hat OpenShift AI and what all you can do there using another Object detection use case*
